{
  "id": "case-27",
  "number": 27,
  "title": "A Tempestade de Retries",
  "subtitle": "Uma requisi√ß√£o falhada se transforma em um milh√£o batendo na porta",
  "brief": {
    "narrative": "Tudo come√ßou com um √∫nico timeout. O Servi√ßo de Registros da DP de Distributia ficou lento durante uma janela de manuten√ß√£o rotineira do banco de dados, causando a falha de algumas requisi√ß√µes vindas do App de Patrulha. Mas em vez de recuar, todos os terminais m√≥veis das viaturas come√ßaram a fazer retry imediatamente ‚Äî e depois retries dos retries. Em segundos, o Servi√ßo de Registros foi soterrado sob uma avalanche de requisi√ß√µes duplicadas, cada timeout gerando mais tr√™s tentativas. O Sargento Backoff observa horrorizado o dashboard mostrando o volume de requisi√ß√µes crescendo exponencialmente. O Servi√ßo de Registros, que estava apenas lento, agora est√° completamente sem resposta. Os oficiais em campo n√£o conseguem consultar registros de suspeitos, verificar placas ou registrar ocorr√™ncias. O sistema que estava mancando agora est√° morto ‚Äî assassinado n√£o pelo problema original, mas pela tentativa de cura.",
    "symptoms": [
      "Os tempos de resposta do Servi√ßo de Registros subiram de 200ms para mais de 30 segundos antes de ficar totalmente sem resposta",
      "Os terminais do App de Patrulha mostram 'Requisi√ß√£o Falhou ‚Äî Tentando novamente...' em loop infinito",
      "O volume de requisi√ß√µes ao Servi√ßo de Registros aumentou 50x em 60 segundos",
      "A CPU do banco de dados estava em 45% antes da tempestade mas disparou para 100% sob a carga de retries"
    ],
    "objective": "Identificar como o comportamento agressivo de retry amplificou uma lentid√£o menor em uma queda total do servi√ßo, e determinar a estrat√©gia correta de retry para prevenir tempestades de retries."
  },
  "diagram": {
    "nodes": [
      {
        "id": "patrol-app",
        "type": "client",
        "label": "Patrol App (x200)",
        "status": "degraded",
        "position": { "x": 100, "y": 50 },
        "inspectable": true,
        "inspectData": {
          "title": "Terminais M√≥veis das Viaturas",
          "logs": [
            "[14:30:01] INFO: Request to Records Service ‚Äî plate lookup #A7X-219",
            "[14:30:04] WARN: Request timed out after 3s ‚Äî retrying immediately (attempt 2/5)",
            "[14:30:04] WARN: Request timed out after 3s ‚Äî retrying immediately (attempt 3/5)",
            "[14:30:04] WARN: Request timed out after 3s ‚Äî retrying immediately (attempt 4/5)",
            "[14:30:05] ERROR: All 5 retries exhausted ‚Äî auto-restarting request cycle",
            "[14:30:05] INFO: New request to Records Service ‚Äî plate lookup #A7X-219 (cycle 2)"
          ],
          "data": {
            "Active Terminals": "200",
            "Retry Policy": "Immediate retry, max 5 attempts, then restart",
            "Retry Delay": "0ms (no backoff)",
            "Avg Requests/Terminal": "~15/min (normally 3/min)",
            "Total Request Rate": "~3000/min (normally 600/min)"
          },
          "status": "Todos os terminais presos em loops de retry. Cada requisi√ß√£o falhada dispara 5 retries imediatos sem delay."
        }
      },
      {
        "id": "api-gateway",
        "type": "server",
        "label": "API Gateway",
        "status": "degraded",
        "position": { "x": 350, "y": 50 },
        "inspectable": true,
        "inspectData": {
          "title": "API Gateway",
          "logs": [
            "[14:30:01] INFO: Forwarding requests to Records Service ‚Äî queue depth normal",
            "[14:30:03] WARN: Request queue depth increasing ‚Äî 150 pending",
            "[14:30:05] WARN: Request queue depth critical ‚Äî 2,400 pending",
            "[14:30:08] ERROR: Connection pool to Records Service exhausted",
            "[14:30:10] ERROR: Dropping incoming requests ‚Äî queue overflow"
          ],
          "data": {
            "Request Queue": "OVERFLOW (max 5000)",
            "Connection Pool": "EXHAUSTED (500/500 in use)",
            "Requests Dropped": "1,847 in last 60s",
            "Rate Limiting": "NONE CONFIGURED",
            "Retry-After Header": "NOT SENT"
          },
          "status": "Gateway sobrecarregado pelo tr√°fego de retries. Nenhum rate limiting ou backpressure configurado."
        }
      },
      {
        "id": "records-service",
        "type": "server",
        "label": "Records Service",
        "status": "failed",
        "position": { "x": 600, "y": 50 },
        "inspectable": true,
        "inspectData": {
          "title": "Servi√ßo de Registros",
          "logs": [
            "[14:29:55] INFO: Database maintenance starting ‚Äî expect slight latency increase",
            "[14:30:00] WARN: Response time degraded ‚Äî avg 800ms (threshold: 500ms)",
            "[14:30:05] ERROR: Thread pool saturated ‚Äî 200/200 threads busy",
            "[14:30:08] ERROR: Request timeout rate 100% ‚Äî service effectively down",
            "[14:30:15] ERROR: OOM warning ‚Äî excessive connection objects in memory"
          ],
          "data": {
            "Status": "UNRESPONSIVE",
            "Thread Pool": "200/200 (saturated)",
            "Avg Response Time": "TIMEOUT (>30s)",
            "Original Issue": "Routine DB maintenance ‚Äî 2x latency increase",
            "Current Issue": "50x normal request volume from retry storm"
          },
          "status": "O servi√ßo estava apenas lento devido √† manuten√ß√£o do banco, mas agora est√° completamente fora do ar sob a carga da tempestade de retries."
        }
      },
      {
        "id": "records-db",
        "type": "database",
        "label": "Records Database",
        "status": "degraded",
        "position": { "x": 600, "y": 300 },
        "inspectable": true,
        "inspectData": {
          "title": "Banco de Dados de Registros",
          "logs": [
            "[14:29:55] INFO: Index rebuild started on warrants table",
            "[14:30:00] WARN: Query latency increased during maintenance ‚Äî expected",
            "[14:30:06] ERROR: Connection limit reached ‚Äî 500/500 connections",
            "[14:30:10] ERROR: New connections refused ‚Äî too many open connections"
          ],
          "data": {
            "CPU Usage": "100% (was 45% pre-storm)",
            "Active Connections": "500/500 (max)",
            "Maintenance Status": "Index rebuild ‚Äî 60% complete",
            "Query Latency": "TIMEOUT (was 400ms during maintenance)"
          },
          "status": "O banco estava lidando bem com a manuten√ß√£o at√© a tempestade de retries saturar todas as conex√µes."
        }
      }
    ],
    "edges": [
      { "id": "e-patrol-gateway", "source": "patrol-app", "target": "api-gateway", "label": "retries (50x volume)", "animated": true, "style": "slow" },
      { "id": "e-gateway-records", "source": "api-gateway", "target": "records-service", "label": "overwhelmed", "animated": false, "style": "broken" },
      { "id": "e-records-db", "source": "records-service", "target": "records-db", "label": "queries", "animated": false, "style": "broken" }
    ]
  },
  "diagnosis": {
    "rootCause": {
      "question": "Qual √© a causa raiz da queda total do Servi√ßo de Registros?",
      "options": [
        {
          "id": "rc-1",
          "text": "A opera√ß√£o de manuten√ß√£o do banco de dados corrompeu os dados do Servi√ßo de Registros, tornando-o completamente inacess√≠vel",
          "correct": false,
          "feedback": "A manuten√ß√£o do banco era uma reconstru√ß√£o de √≠ndice rotineira que causou apenas um pequeno aumento de lat√™ncia. Os dados nunca foram corrompidos. O problema real foi que os clientes reagiram √† lentid√£o com retries agressivos e imediatos que amplificaram a carga muito al√©m do que o servi√ßo conseguia aguentar."
        },
        {
          "id": "rc-2",
          "text": "Retries imediatos agressivos sem backoff amplificaram uma lentid√£o menor em uma sobrecarga catastr√≥fica",
          "correct": true,
          "feedback": "Correto! Esta √© uma tempestade de retries cl√°ssica. O Servi√ßo de Registros estava apenas lento (800ms em vez de 200ms), o que era gerenci√°vel. Mas 200 clientes disparando 5 retries imediatos sem delay transformaram 600 requisi√ß√µes/min em mais de 3000/min. Cada retry que dava timeout gerava mais retries, criando um ciclo de feedback positivo que soterrou o servi√ßo. A cura foi pior que a doen√ßa."
        },
        {
          "id": "rc-3",
          "text": "O API Gateway crashou e ficou completamente fora do ar, pois a aloca√ß√£o de mem√≥ria era insuficiente para o volume de requisi√ß√µes",
          "correct": false,
          "feedback": "O API Gateway ficou sobrecarregado mas n√£o crashou ‚Äî foi uma v√≠tima, n√£o a causa. A quest√£o raiz √© o comportamento de retry dos clientes. Mesmo com um gateway maior, a tempestade de retries ainda teria sobrecarregado o Servi√ßo de Registros."
        },
        {
          "id": "rc-4",
          "text": "Viaturas demais estavam ativas e enviando requisi√ß√µes simultaneamente, ultrapassando em muito a capacidade m√°xima do sistema",
          "correct": false,
          "feedback": "200 viaturas gerando 600 requisi√ß√µes/min estava bem dentro da capacidade normal. O sistema lida com essa carga diariamente. O problema foi que a pol√≠tica de retry de cada viatura multiplicou sua taxa de requisi√ß√µes por 5x ou mais durante a lentid√£o, transformando carga normal em uma tempestade avassaladora."
        }
      ]
    },
    "fix": {
      "question": "Qual √© a melhor estrat√©gia para prevenir tempestades de retries?",
      "options": [
        {
          "id": "fix-1",
          "text": "Desabilitar todos os retries para que requisi√ß√µes falhadas simplesmente falhem sem nenhuma nova tentativa",
          "correct": false,
          "feedback": "Desabilitar retries inteiramente significa que erros transientes (uma breve falha de rede) causariam falhas permanentes. Retries s√£o valiosos ‚Äî o problema √© como s√£o implementados. Voc√™ precisa de retries inteligentes, n√£o de zero retries."
        },
        {
          "id": "fix-2",
          "text": "Implementar backoff exponencial com jitter para que os retries sejam espa√ßados com delays aleat√≥rios",
          "correct": true,
          "feedback": "Correto! Backoff exponencial (esperar 1s, depois 2s, depois 4s, etc.) evita o efeito manada distribuindo os retries ao longo do tempo. Adicionar jitter (varia√ß√£o aleat√≥ria) evita ondas sincronizadas de retry onde todos os clientes fazem retry no mesmo instante. Combinado com um n√∫mero m√°ximo razo√°vel de retries, isso permite que problemas transientes se resolvam naturalmente sem amplificar a carga."
        },
        {
          "id": "fix-3",
          "text": "Aumentar o pool de threads do Servi√ßo de Registros de 200 para 2000 para suportar mais requisi√ß√µes simult√¢neas",
          "correct": false,
          "feedback": "Mais threads adiariam o colapso mas n√£o o preveniriam. Uma tempestade de retries cresce exponencialmente ‚Äî se voc√™ aguenta 10x de carga, a tempestade simplesmente crescer√° para 100x. Voc√™ precisa resolver o comportamento de retry em si, n√£o tentar absorver tr√°fego ilimitado de retries."
        },
        {
          "id": "fix-4",
          "text": "Adicionar um load balancer com mais inst√¢ncias do Servi√ßo de Registros para distribuir o tr√°fego",
          "correct": false,
          "feedback": "Escalar horizontalmente ajuda com crescimento normal de carga, mas uma tempestade de retries √© um ciclo de feedback positivo ‚Äî mais capacidade apenas significa que demora um pouco mais para sobrecarregar. Cada retry que d√° timeout ainda gera mais retries. A corre√ß√£o fundamental deve endere√ßar o comportamento de retry no n√≠vel do cliente."
        }
      ]
    }
  },
  "conceptId": "retry-strategies",
  "badge": {
    "name": "Ca√ßador de Tempestades",
    "icon": "üå™Ô∏è"
  }
}